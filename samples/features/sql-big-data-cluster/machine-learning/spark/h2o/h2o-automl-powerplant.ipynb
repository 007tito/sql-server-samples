{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1543381571657_0002</td><td>pyspark3</td><td>idle</td><td><a target=\"_blank\" href=\"https://10.193.17.116:30443/gateway/default/yarn/proxy/application_1543381571657_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://mssql-storage-pool-default-0.service-storage-pool-default.test.svc.cluster.local:8042/node/containerlogs/container_1543381571657_0002_01_000001/root\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting h2o_pysparkling_2.3\n",
      "  Downloading https://files.pythonhosted.org/packages/e2/2d/43ea2377f68a072161c0dc05a90d722e9b391bac6bb28ac0f54fb1bdafbf/h2o_pysparkling_2.3-2.3.18.tar.gz (51.2MB)\n",
      "Requirement already satisfied (use --upgrade to upgrade): requests in /usr/local/lib/python3.5/dist-packages (from h2o_pysparkling_2.3)\n",
      "Collecting tabulate (from h2o_pysparkling_2.3)\n",
      "  Downloading https://files.pythonhosted.org/packages/12/c2/11d6845db5edf1295bc08b2f488cf5937806586afe42936c3f34c097ebdc/tabulate-0.8.2.tar.gz (45kB)\n",
      "Collecting future (from h2o_pysparkling_2.3)\n",
      "  Downloading https://files.pythonhosted.org/packages/90/52/e20466b85000a181e1e144fd8305caf2cf475e2f9674e797b222f8105f5f/future-0.17.1.tar.gz (829kB)\n",
      "Collecting colorama>=0.3.8 (from h2o_pysparkling_2.3)\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied (use --upgrade to upgrade): six in /usr/local/lib/python3.5/dist-packages (from h2o_pysparkling_2.3)\n",
      "Collecting pyspark<=2.3.2,>=2.3.0 (from h2o_pysparkling_2.3)\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/cb/d8ff49ba885e2c88b8cf2967edd84235ffa9ac301bffef657dfa5605a112/pyspark-2.3.2.tar.gz (211.9MB)\n",
      "Requirement already satisfied (use --upgrade to upgrade): certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests->h2o_pysparkling_2.3)\n",
      "Requirement already satisfied (use --upgrade to upgrade): chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests->h2o_pysparkling_2.3)\n",
      "Requirement already satisfied (use --upgrade to upgrade): urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests->h2o_pysparkling_2.3)\n",
      "Requirement already satisfied (use --upgrade to upgrade): idna<2.8,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests->h2o_pysparkling_2.3)\n",
      "Collecting py4j==0.10.7 (from pyspark<=2.3.2,>=2.3.0->h2o_pysparkling_2.3)\n",
      "  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
      "Building wheels for collected packages: h2o-pysparkling-2.3, tabulate, future, pyspark\n",
      "  Running setup.py bdist_wheel for h2o-pysparkling-2.3: started\n",
      "  Running setup.py bdist_wheel for h2o-pysparkling-2.3: finished with status 'done'\n",
      "  Stored in directory: /home/.cache/pip/wheels/0c/cd/e6/65d3a3c52525dc1ea3d13ad17e0935172db2f0dcd4a08322ba\n",
      "  Running setup.py bdist_wheel for tabulate: started\n",
      "  Running setup.py bdist_wheel for tabulate: finished with status 'done'\n",
      "  Stored in directory: /home/.cache/pip/wheels/2a/85/33/2f6da85d5f10614cbe5a625eab3b3aebfdf43e7b857f25f829\n",
      "  Running setup.py bdist_wheel for future: started\n",
      "  Running setup.py bdist_wheel for future: finished with status 'done'\n",
      "  Stored in directory: /home/.cache/pip/wheels/0c/61/d2/d6b7317325828fbb39ee6ad559dbe4664d0896da4721bf379e\n",
      "  Running setup.py bdist_wheel for pyspark: started\n",
      "  Running setup.py bdist_wheel for pyspark: finished with status 'done'\n",
      "  Stored in directory: /home/.cache/pip/wheels/be/7d/34/cd3cfbc75d8b6b6ae0658e5425348560b86d187fe3e53832cc\n",
      "Successfully built h2o-pysparkling-2.3 tabulate future pyspark\n",
      "Installing collected packages: tabulate, future, colorama, py4j, pyspark, h2o-pysparkling-2.3\n",
      "Successfully installed colorama-0.4.1 future-0.17.1 h2o-pysparkling-2.3-2.3.18 py4j-0.10.7 pyspark-2.3.2 tabulate-0.8.2\n",
      "You are using pip version 8.1.1, however version 18.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command."
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Install H2O PySparkling\n",
    "stdout = subprocess.check_output(\n",
    "    \"pip3 install h2o_pysparkling_2.3\",\n",
    "    stderr=subprocess.STDOUT,\n",
    "    shell=True).decode(\"utf-8\")\n",
    "print(stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: `/tmp/powerplant_output.csv': No such file or directory\n",
      "--2018-11-28 18:21:03--  https://raw.githubusercontent.com/h2oai/h2o-tutorials/master/h2o-world-2017/automl/data/powerplant_output.csv\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 308777 (302K) [text/plain]\n",
      "Saving to: 'powerplant_output.csv'\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 16%  293K 1s\n",
      "    50K .......... .......... .......... .......... .......... 33%  568K 1s\n",
      "   100K .......... .......... .......... .......... .......... 49%  294K 0s\n",
      "   150K .......... .......... .......... .......... .......... 66%  228K 0s\n",
      "   200K .......... .......... .......... .......... .......... 82%  193K 0s\n",
      "   250K .......... .......... .......... .......... .......... 99%  231K 0s\n",
      "   300K .                                                     100% 2937G=1.1s\n",
      "\n",
      "2018-11-28 18:21:05 (268 KB/s) - 'powerplant_output.csv' saved [308777/308777]"
     ]
    }
   ],
   "source": [
    "dataFileName = \"powerplant_output.csv\"\n",
    "dataFileUrl = \"https://raw.githubusercontent.com/h2oai/h2o-tutorials/master/h2o-world-2017/automl/data/\" + dataFileName\n",
    "\n",
    "# Download data file and copy to HDFS, if not already there\n",
    "cmd = 'hdfs dfs -ls /tmp/' + dataFileName + ' || ' \\\n",
    "    '(wget ' + dataFileUrl + ' && ' \\\n",
    "    'hdfs dfs -copyFromLocal ' + dataFileName + ' /tmp && ' \\\n",
    "    'rm ' + dataFileName + ')'\n",
    "\n",
    "stdout = subprocess.check_output(\n",
    "    cmd,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    shell=True).decode(\"utf-8\")\n",
    "print(stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://10.244.0.66:54323... successful.\n",
      "--------------------------  ---------------------------------------------------\n",
      "H2O cluster uptime:         12 secs\n",
      "H2O cluster timezone:       Etc/UTC\n",
      "H2O data parsing timezone:  UTC\n",
      "H2O cluster version:        3.22.0.2\n",
      "H2O cluster version age:    6 days\n",
      "H2O cluster name:           sparkling-water-root_application_1543381571657_0002\n",
      "H2O cluster total nodes:    3\n",
      "H2O cluster free memory:    3.698 Gb\n",
      "H2O cluster total cores:    48\n",
      "H2O cluster allowed cores:  3\n",
      "H2O cluster status:         accepting new members, healthy\n",
      "H2O connection url:         http://10.244.0.66:54323\n",
      "H2O connection proxy:\n",
      "H2O internal security:      False\n",
      "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
      "Python version:             3.5.2 final\n",
      "--------------------------  ---------------------------------------------------\n",
      "\n",
      "Sparkling Water Context:\n",
      " * H2O name: sparkling-water-root_application_1543381571657_0002\n",
      " * cluster size: 3\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (1,mssql-storage-pool-default-1.service-storage-pool-default.test.svc.cluster.local,54321)\n",
      "  (2,mssql-storage-pool-default-0.service-storage-pool-default.test.svc.cluster.local,54321)\n",
      "  (3,mssql-storage-pool-default-1.service-storage-pool-default.test.svc.cluster.local,54323)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://10.244.0.66:54323 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n",
      " * Yarn App ID of Spark application: application_1543381571657_0002"
     ]
    }
   ],
   "source": [
    "from pysparkling import H2OContext\n",
    "\n",
    "hc = H2OContext.getOrCreate(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mssql-storage-pool-default-0"
     ]
    }
   ],
   "source": [
    "# Print the hostname\n",
    "stdout = subprocess.check_output(\n",
    "    \"hostname\",\n",
    "    stderr=subprocess.STDOUT,\n",
    "    shell=True).decode(\"utf-8\")\n",
    "print(stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+-----------------------+----------------+--------------------+--------------------+\n",
      "|TemperatureCelcius|ExhaustVacuumHg|AmbientPressureMillibar|RelativeHumidity|HourlyEnergyOutputMW|   prediction_output|\n",
      "+------------------+---------------+-----------------------+----------------+--------------------+--------------------+\n",
      "|             10.01|          41.17|                1018.78|           86.84|               479.4|[478.98443473089264]|\n",
      "|             10.02|          39.66|                1016.34|           79.98|              480.05| [478.4744401166399]|\n",
      "|             10.03|          43.13|                1014.85|           70.09|              482.16|[475.98085258410845]|\n",
      "|             10.04|          41.62|                1013.36|           95.17|              463.87|[468.36092209033416]|\n",
      "|             10.05|          41.58|                1021.35|           95.19|              469.03|[468.67240194913506]|\n",
      "|             10.06|          34.69|                 1027.9|           71.73|              477.68|[477.79641646128107]|\n",
      "|             10.08|          37.92|                1010.47|           66.37|              474.63| [475.2033972206058]|\n",
      "|             10.08|          41.16|                1023.14|           96.03|              469.17| [470.3570481951197]|\n",
      "|             10.09|          41.01|                1019.89|           96.55|              471.15| [469.6710775300654]|\n",
      "|              10.1|           41.4|                1024.29|           85.94|              474.28|[477.42433033233783]|\n",
      "|             10.11|          39.35|                1015.19|           90.74|              479.83|  [477.792144625552]|\n",
      "|             10.11|          39.72|                 1019.1|           69.68|               476.8|[473.80793646542656]|\n",
      "|             10.11|          42.49|                1010.22|           82.11|              483.56| [476.5521742794438]|\n",
      "|             10.12|          41.55|                1005.78|           62.34|              475.46| [475.9253245215578]|\n",
      "|             10.12|          41.78|                1013.43|           73.47|              477.67| [475.0193582197604]|\n",
      "|             10.13|          39.18|                1024.09|           85.48|              479.42| [477.9988789711212]|\n",
      "|             10.15|          39.22|                1020.09|           68.75|              474.87| [476.4446239317696]|\n",
      "|             10.15|          41.46|                1019.78|           83.56|              481.31| [479.5577986068803]|\n",
      "|             10.15|          43.41|                 1018.4|           82.07|              473.43|[476.77545876514284]|\n",
      "|             10.16|           39.3|                1019.71|           81.21|              480.74| [477.2729727910962]|\n",
      "+------------------+---------------+-----------------------+----------------+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pysparkling.ml import H2OAutoML\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import SQLTransformer\n",
    "\n",
    "powerplant_df = spark.read.option(\"inferSchema\", \"true\").csv(\"/tmp/powerplant_output.csv\", header=True)\n",
    "\n",
    "splits = powerplant_df.randomSplit([0.8, 0.2], seed=1)\n",
    "train = splits[0]\n",
    "for_predictions = splits[1]\n",
    "\n",
    "temperatureTransformer = SQLTransformer(statement=\"SELECT * FROM __THIS__ WHERE TemperatureCelcius > 10\")\n",
    "\n",
    "automlEstimator = H2OAutoML(maxModels=2, predictionCol=\"HourlyEnergyOutputMW\",\n",
    "    ratio=0.9, seed=1)\n",
    "\n",
    "pipeline = Pipeline(stages=[temperatureTransformer, automlEstimator])\n",
    "model = pipeline.fit(train)\n",
    "predicted = model.transform(for_predictions)\n",
    "\n",
    "predicted.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "|model_id                                           |mean_residual_deviance|rmse              |mse               |mae               |rmsle                |\n",
      "+---------------------------------------------------+----------------------+------------------+------------------+------------------+---------------------+\n",
      "|StackedEnsemble_BestOfFamily_AutoML_20181128_003911|11.00711037433692     |3.317696546451607 |11.00711037433692 |2.460978645004018 |0.0073362304526039795|\n",
      "|StackedEnsemble_AllModels_AutoML_20181128_003911   |11.00711037433692     |3.317696546451607 |11.00711037433692 |2.460978645004018 |0.0073362304526039795|\n",
      "|XRT_1_AutoML_20181128_003911                       |11.18040320029184     |3.3437109923394757|11.18040320029184 |2.5010207855031896|0.007393792671247257 |\n",
      "|DRF_1_AutoML_20181128_003911                       |11.245178401907951    |3.3533831278140513|11.245178401907951|2.4970917585784322|0.007413255513108019 |\n",
      "+---------------------------------------------------+----------------------+------------------+------------------+------------------+---------------------+"
     ]
    }
   ],
   "source": [
    "automlEstimator.leaderboard().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "scores = predicted.select(predicted['HourlyEnergyOutputMW'], predicted['prediction_output']['value'].alias('prediction'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 2.4058999799874976"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                labelCol=\"HourlyEnergyOutputMW\",\n",
    "                                metricName=\"mae\")\n",
    "\n",
    "mae = evaluator.evaluate(scores)\n",
    "\n",
    "print(\"Mean absolute error:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r squared: 0.9516895664346249"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\",\n",
    "                                labelCol=\"HourlyEnergyOutputMW\",\n",
    "                                metricName=\"r2\")\n",
    "\n",
    "r2 = evaluator.evaluate(scores)\n",
    "\n",
    "print(\"r squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 199.5 seconds."
     ]
    }
   ],
   "source": [
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {:.1f} seconds.\".format(elapsed_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark3",
   "language": "",
   "name": "pyspark3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark3",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
